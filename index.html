<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }

  /* added by yc */

  padding {
    padding: 50px 0px;
  }

  </style>
  <link rel="icon" type="image/png" href="img/icon.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Yifan Wang (王怿凡)</title>
  <meta name="Yifan Wang's Homepage" http-equiv="Content-Type" content="Yifan Wang's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <script src="js/scramble.js"></script>
</head>

<body>
<!-- <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20"> -->
<table width="960" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Yifan Wang (王怿凡)</pageheading><br>
    <b>email</b>: yifan.wang.5 _at_ stonybrook.edu
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
  </p>

  <tr>
    <td width="40%" valign="top"><a href="img/profile.png"><img src="img/profile.png" width="100%" style="border-radius:15px"></a>
        <p align=center>
          | <a href="data/CV_25_07_31.pdf">CV</a> |
          <a href="https://scholar.google.com/citations?hl=en&user=T8kpcuEAAAAJ">Google Scholar</a> |
          <a href="https://github.com/Zc0in/">Github</a> |
          <a href="https://www.linkedin.com/in/yifan-wang-92a593249/">LinkedIn</a> |
      </p>
    </td>

    <td width="66%" valign="top" align="justify">
        <p>
          Hi it's Yifan. I am a second-year CS Ph.D. student in Stony Brook University, advised by 
          <a href="https://chenyuyou.me/">Chenyu You</a>.
          My research interest broadly lies in <b>Computer Vision</b>, <b>Machine Learning</b> and <b>Cognitive Science</b>.
          I am intrigued by the intersection of cognitive science and machine learning and I am committed to the development of reliable machine learning systems.
        <br>
        <br>

        Previously, I graduated from ShanghaiTech University with a major in computer science, advised by 
        <a href="https://www.saying.ren/">Kan Ren</a>. 
        I also spent a wonderful year at UC Berkeley as an exchanged junior, where I worked as a research intern in <a href="https://whitneylab.berkeley.edu/">Whitney's Lab</a>.

        <!-- <br>
        <br>

        <b>
	      I am actively seeking for a research intern for 2025 Summer. 
        Please feel free to reach out to me via email if you believe I am a good fit for your research team. 
        I welcome the opportunity for further discussion! Please see my 
        <a href="data/CV_24_11_6.pdf">CV</a> for more details. -->
	</b>

        </p>

        </td>
  </tr>
</table>

<!-- =================== Experience =================== -->
<!-- <table width="100%" align="center" style="margin-left:10px" cellspacing="0" cellpadding="0" border="0">
    <tr>
        <th width="16.7%" valign="top" align="center">
        <img src="images/uiuc_wordmark.png" alt="sym" width="100%" style="float: left; padding: 25px 3px 44px 0px;"></a>
        <p style="line-height:1.3; font-size:12pt">UIUC<br>PhD in CS<br>Aug. 22 - Present</p>
        </th>

        <th width="16.6%" valign="top" align="center">
          <img src="images/adobe.png" alt="sym" width="60%"></a>
          <p style="line-height:1.3; font-size:12pt">Adobe<br>Research Intern<br>May. 24 - Present</p>
          </th>

        <th width="16.6%" valign="top" align="center">
          <img src="images/cmu.png" alt="sym" width="90%" style="padding: 3px 0px 0px 0px"></a>
          <p style="line-height:1.3; font-size:12pt">CMU<br>MS in Computer Vision<br>Jan. 21 - May. 22</p>
        </th>

        <th width="16.6%" valign="top" align="center">
        <img src="images/snap_logo.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">Snap Inc.<br>Research Intern<br>May. 21 - Aug. 21</p>
        </th>

        <th width="16.6%" valign="top" align="center">
        <img src="images/microsoft.png" alt="sym" width="80%"></a>
        <p style="line-height:1.3; font-size:12pt">Microsoft<br>Research Intern<br>Mar. 20 - July 20</p>
        </th>

        <th width="16.6%" valign="top" align="center">
        <img src="images/ucm.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">UC Merced<br>Visiting Scholar<br>Sept. 19 - Mar. 20</p>
        </th>
    </tr>
</table> -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="8">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
      <li> [12/2025] Thrilled to be recognized as IEEE Transactions on Medical Imaging Bronze Distinguished Reviewer! </li>
      <li> [06/2025] Our paper <a href="https://arxiv.org/abs/2508.14461">Ouroboros</a> has been accepted by ICCV 2025! </li>
      <li> [06/2025] Our paper <a href="https://arxiv.org/abs/2506.20741">OTSurv</a> has been accepted by MICCAI 2025! </li>
      <li> [05/2025] I will join NEC Labs America this summer as a research intern with <a href="https://geekjzy.github.io/">Ziyu Jiang</a>! </li>
      <li> [02/2025] Our paper <a href="https://foundation-model-research.github.io/difflens/">DiffLens</a> has been accepted by CVPR 2025! </li>
      <li> [01/2025] Our paper <a href="https://openreview.net/forum?id=WQQyJbr5Lh">Neuron Path</a> has been accepted by ICLR 2025! </li>
      <li> [08/2024] I will join Stony Brook University in 24Fall as a CS PhD student! </li>
      <li> [03/2024] Our paper <a href="https://arxiv.org/abs/2401.10278">EEGFormer</a> has been accepted by AAAI 2024 SSS on Clinical FMs! </li>
      <li> [10/2023] Our paper <a href="https://veatic.github.io/">VEATIC Dataset</a> has been accepted by WACV 2024! </li>
      <li> [06/2023] I completed the one-year exchange in UC Berkeley! </li>
    </ul>
  </td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/ouroboros.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2508.14461">

            <heading>Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering</heading></a>
            <br>
            Shanlin Sun<sup>*</sup>,
            <b>Yifan Wang</b><sup>*</sup>,
            Hanwen Zhang<sup>*</sup>,
            Yifeng Xiong,
            Qin Ren,
            Ruogu Fang,
            Xiaohui Xie,
            Chenyu You
            (* Equal contribution)
            <br>
            <em>ICCV</em>, 2025
        </p>

        <div class="paper" id="Ouroboros">
        <a href="https://siwensun.github.io/ouroboros-project/">webpage</a> |
        <a href="javascript:toggleblock('Ouroboros_abs')">abstract</a> |
        <a href="https://arxiv.org/abs/2508.14461">paper</a> |
        <a href="https://github.com/Y-Research-SBU/Ouroboros">code</a> |

        <p align="justify">
            <i id="Ouroboros_abs">
              While multi-step diffusion models have advanced both forward and inverse rendering, 
              existing approaches often treat these problems independently, 
              leading to cycle inconsistency and slow inference speed. 
              In this work, we present Ouroboros, a framework composed of two single-step diffusion models that handle forward and inverse rendering with mutual reinforcement. 
              Our approach extends intrinsic decomposition to both indoor and outdoor scenes and introduces a cycle consistency mechanism that 
              ensures coherence between forward and inverse rendering outputs. Experimental results demonstrate state-of-the-art performance 
              across diverse scenes while achieving substantially faster inference speed compared to other diffusion-based methods. 
              We also demonstrate that Ouroboros can transfer to video decomposition in a training-free manner, 
              reducing temporal inconsistency in video sequences while maintaining high-quality per-frame inverse rendering.
            </i>
        </p>

        </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/OTSurv_main.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2506.20741">

            <heading>OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport</heading></a>
            <br>
            Qin Ren,
            <b>Yifan Wang</b>,
            Ruogu Fang,
            Haibin Ling,
            Chenyu You
            <br>
            <em>MICCAI</em>, 2025
        </p>

        <div class="paper" id="OTSurv">
        <!-- <a href="https://yfwang.me/">webpage</a> | -->
        <a href="javascript:toggleblock('OTSurv_abs')">abstract</a> |
        <a href="https://arxiv.org/abs/2506.20741/">paper</a> |
        <a href="https://github.com/Y-Research-SBU/OTSurv">code</a> |

        <p align="justify">
            <i id="OTSurv_abs">
              Survival prediction using whole slide images (WSIs) can be formulated as a multiple instance learning (MIL) problem. 
              However, existing MIL methods often fail to explicitly capture pathological heterogeneity within WSIs, both globally -- 
              through long-tailed morphological distributions, and locally -- through tile-level prediction uncertainty. 
              Optimal transport (OT) provides a principled way of modeling such heterogeneity by incorporating marginal distribution constraints. 
              Building on this insight, we propose OTSurv, a novel MIL framework from an optimal transport perspective. 
              Specifically, OTSurv formulates survival predictions as a heterogeneity-aware OT problem with two constraints: 
              (1) global long-tail constraint that models prior morphological distributions to avert both mode collapse and 
              excessive uniformity by regulating transport mass allocation, and (2) local uncertainty-aware constraint 
              that prioritizes high-confidence patches while suppressing noise by progressively raising the total transport mass.  
              We then recast the initial OT problem, augmented by these constraints, into an unbalanced OT formulation that can be solved with an efficient, 
              hardware-friendly matrix scaling algorithm. Empirically, OTSurv sets new state-of-the-art results across six popular benchmarks, 
              achieving an absolute 3.6% improvement in average C-index. In addition, OTSurv achieves statistical significance in log-rank tests and offers high interpretability, 
              making it a powerful tool for survival prediction in digital pathology. 
              Our codes are available at <a href="https://github.com/Y-Research-SBU/OTSurv">https://github.com/Y-Research-SBU/OTSurv</a>.
            </i>
        </p>

        </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/diffbias00.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://foundation-model-research.github.io/difflens/">

            <heading>Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability</heading></a>
            <br>
            Yingdong Shi<sup>*</sup>,
            Changming Li<sup>*</sup>,
            <b>Yifan Wang</b>,
            Yongxiang Zhao,
            Anqi Pang,
            Sibei Yang,
            Jingyi Yu,
            Kan Ren
            <br>
            <em>CVPR</em>, 2025
        </p>

        <div class="paper" id="DiffLen">
        <a href="https://foundation-model-research.github.io/difflens/">webpage</a> |
        <a href="javascript:toggleblock('DiffLen_abs')">abstract</a> |
        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Shi_Dissecting_and_Mitigating_Diffusion_Bias_via_Mechanistic_Interpretability_CVPR_2025_paper.pdf">paper</a> |
        <a href="https://github.com/foundation-model-research/DiffLens">code</a> |

        <p align="justify">
            <i id="DiffLen_abs">
              Diffusion models have demonstrated impressive capabilities in synthesizing diverse content. 
              However, despite their high-quality outputs, these models often perpetuate social biases, including those related to gender and race.
              These biases can potentially contribute to harmful real-world consequences, reinforcing stereotypes and exacerbating inequalities in various social contexts.
              While existing research on diffusion bias mitigation has predominantly focused on guiding content generation, it often neglects the intrinsic mechanisms within diffusion models that causally drive biased outputs. 
              In this paper, we investigate the internal processes of diffusion models, identifying specific decision-making mechanisms, termed bias features, embedded within the model architecture.
              By directly manipulating these features, our method precisely isolates and adjusts the elements responsible for bias generation, permitting granular control over the bias levels in the generated content.
              Through experiments on both unconditional and conditional diffusion models across various social bias attributes, we demonstrate our method's efficacy in managing generation distribution while preserving image quality.
              We also dissect the discovered model mechanism, revealing different intrinsic features controlling fine-grained aspects of generation, boosting further research on mechanistic interpretability of diffusion models.
              The project website is at <a href="https://foundation-model-research.github.io/difflens">https://foundation-model-research.github.io/difflens</a>.
            </i>
        </p>

        </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/iclr25.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://openreview.net/forum?id=WQQyJbr5Lh">

            <heading>Discovering Influential Neuron Path in Vision Transformers</heading></a>
            <br>
            <b>Yifan Wang</b>,
              Yifei Liu,
              Yingdong Shi,
              Changming Li,
              Anqi Pang,
              Sibei Yang,
              Jingyi Yu,
              Kan Ren
            <br>
            <em>ICLR</em>, 2025
        </p>

        <div class="paper" id="iclr_25">
        <a href="javascript:toggleblock('iclr_25_abs')">abstract</a> |
        <a href="https://openreview.net/forum?id=WQQyJbr5Lh">paper</a> |
        <a href="https://github.com/foundation-model-research/NeuronPath">code</a> |

        <p align="justify">
            <i id="iclr_25_abs">
              Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. 
              While prior research has attempted to demystify these models through input attribution and neuron role analysis,
              there's been a notable gap in considering layer-level information and the holistic path of information flow across layers.
              In this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly.
              We first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome.
              And we further provide a 
              layer-progressive neuron locating
              approach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model.
              Our experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions.
              Additionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. 
              We further analyze the key effects of these neurons on the image classification task, showcasing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning.
              The project website including implementation code is available at <a href="https://foundation-model-research.github.io/NeuronPath/">https://foundation-model-research.github.io/NeuronPath/</a>.
            </i>
        </p>

        </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/vqpatch.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2401.10278">

            <heading>EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model</heading></a>
            <br>
            Yuqi Chen,
            Kan Ren,
            Kaitao Song,
            Yansen Wang,
            <b>Yifan Wang</b>,
            Dongsheng Li,
            Lili Qiu
            <br>
            <em>AAAI</em>, 2024 SSS on Clinical FMs
        </p>

        <div class="paper" id="eegformer">
        <a href="javascript:toggleblock('eegformer_abs')">abstract</a> |
        <a href="https://arxiv.org/abs/2401.10278">paper</a> |

        <p align="justify">
            <i id="eegformer_abs">
              Self-supervised learning has emerged as a highly effective approach in the fields of 
              natural language processing and computer vision. It is also applicable to brain signals such as 
              electroencephalography (EEG) data, given the abundance of available unlabeled data that exist 
              in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. 
              The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon 
              each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant 
              data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on 
              end-to-end model learning which is not easy for humans to understand. In this paper, 
              we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. 
              The pretrained model cannot only learn universal representations on EEG signals with adaptable performance on 
              various downstream tasks but also provide interpretable outcomes of the useful patterns within the data. 
              To validate the effectiveness of our model, we extensively evaluate it on various downstream tasks and assess 
              the performance under different transfer settings. Furthermore, we demonstrate how the learned model exhibits 
              transferable anomaly detection performance and provides valuable interpretability of the acquired patterns 
              via self-supervised learning.
            </i>
        </p>

        </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/veatic.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://veatic.github.io/">

            <heading>VEATIC: Video-based Emotion and Affect Tracking in Context Dataset</heading></a>
            <br>
            Zhihang Ren<sup>*</sup>,
            Jefferson Ortega<sup>*</sup>,
            <b>Yifan Wang</b><sup>*</sup>,
            Zhimin Chen,
            David Whitney,
            Yunhui Guo,
            Stella Yu
            (* Equal contribution)
            <br>
            <em>WACV</em>, 2024
        </p>

        <div class="paper" id="Veatic">
        <a href="https://veatic.github.io/">webpage</a> |
        <a href="javascript:toggleblock('Veatic_abs')">abstract</a> |
        <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Ren_VEATIC_Video-Based_Emotion_and_Affect_Tracking_in_Context_Dataset_WACV_2024_paper.pdf">paper</a> |
        <a href="https://github.com/AlbusPeter/VEATIC">code</a> |

        <p align="justify">
            <i id="Veatic_abs">
              Human affect recognition has been a significant topic in psychophysics and computer vision. 
              However, the currently published datasets have many limitations. For example, most datasets contain frames 
              that contain only information about facial expressions. Due to the limitations of previous datasets, 
              it is very hard to either understand the mechanisms for affect recognition of humans or generalize well on 
              common cases for computer vision models trained on those datasets. In this work, we introduce a brand new 
              large dataset, the Video-based Emotion and Affect Tracking in Context Dataset (VEATIC), that can conquer 
              the limitations of the previous datasets. VEATIC has 124 video clips from Hollywood movies, documentaries, 
              and home videos with continuous valence and arousal ratings of each frame via real-time annotation. 
              Along with the dataset, we propose a new computer vision task to infer the affect of the selected character 
              via both context and character information in each video frame. Additionally, we propose a simple model to 
              benchmark this new computer vision task. We also compare the performance of the pretrained model using our 
              dataset with other similar datasets. Experiments show the competing results of our pretrained model via VEATIC, 
              indicating the generalizability of VEATIC.
            </i>
        </p>

        </div>
    </td>
  </tr>

<table width="100%" align="center" border="0" cellpadding="10">
  <tr>
    <td>
      <sectionheading>&nbsp;&nbsp;Professional Activity</sectionheading>
        <ul>
          <li> Conference Reviewer: CVPR 2025-2026, ICLR 2026, NeurIPS 2025, MICCAI 2025</li>
          <li> Journal Reviewer: TCSVT, MedIA, Pattern Recognition, TMI, TNNLS</li>
          <li> Teaching Assistant: CSE 549 (SBU), IAE 101 (SBU)</li>
        </ul>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellpadding="10">
  <tr>
    <td>
      <sectionheading>&nbsp;&nbsp;Awards</sectionheading>
        <ul>
          <li> [12/2025] I received the honor of being IEEE Transactions on Medical Imaging Bronze Distinguished Reviewer. </li>
          <li> [06/2024] I received the honor of being the <b>Outstanding Graduate</b> in ShanghaiTech. </li>
          <li> [12/2023] I received the honor of being 2022-2023 <b>Merit Student</b> in ShanghaiTech. </li>
          <li> [07/2023] I received the <b>Undergraduate International Exchange Special Scholarship</b> in ShanghaiTech. </li>
          <li> [12/2022] I received the honor of being 2021-2022 <b>Merit Student</b> in ShanghaiTech. </li>
        </ul>
    </td>
  </tr>
</table>

<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=n&d=5Mbudn0FSMUmDjHq2NxUjvoq_vySIg_giud-A4yWPHk"></script>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr><td><br><p align="center"><font size="2">
    Template from this <a href="https://jonbarron.info/">awesome website</a>.
    </font></p></td></tr>
    
</table>
  </td> </tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('Ouroboros_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('OTSurv_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('DiffLen_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('iclr_25_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('eegformer_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('Veatic_abs');
</script>

</script>
</body>

</html>