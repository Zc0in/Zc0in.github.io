<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="HorizonForge: Driving Scene Editing with Any Trajectories and Any Vehicles">
  <meta name="keywords" content="HorizonForge, driving scene editing, trajectory editing, controllable video, ego vehicle, multi-agent">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HorizonForge: Driving Scene Editing with Any Trajectories and Any Vehicles</title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
</head>
<body class="hf-app-active">
<button id="theme-toggle" type="button" class="theme-toggle-btn"></button>

<div id="hf-app">
  <section class="hf-hero">
    <div class="container">
      <h1 class="hf-title">HorizonForge: Driving Scene Editing with Any Trajectories and Any Vehicles</h1>
      <!-- <p class="hf-tagline">Camera-ready project page showcasing controllable driving-scene edits across ego maneuvers and multi-agent operations.</p> -->
      <div class="hf-author-block">
        <p class="hf-author-list">
          <a href="https://yfwang.me/" target="_blank" rel="noopener noreferrer">Yifan Wang</a><sup>1, 2</sup>
          <span>路</span>
          <a href="https://www.francescopittaluga.com/" target="_blank" rel="noopener noreferrer">Francesco Pittaluga</a><sup>1</sup>
          <span>路</span>
          <a href="https://zaidtas.github.io/" target="_blank" rel="noopener noreferrer">Zaid Tasneem</a><sup>1</sup>
          <span>路</span>
          <a href="https://chenyuyou.me/" target="_blank" rel="noopener noreferrer">Chenyu You</a><sup>2</sup>
          <span>路</span>
          <a href="https://cseweb.ucsd.edu/~mkchandraker/" target="_blank" rel="noopener noreferrer">Manmohan Chandraker</a><sup>1, 3</sup>
          <span>路</span>
          <a href="https://geekjzy.github.io/" target="_blank" rel="noopener noreferrer">Ziyu Jiang</a><sup>1</sup>
        </p>
        <p class="hf-affiliations">
          <span><sup>1</sup> NEC Labs America</span>
          <span><sup>2</sup> Stony Brook University</span>
          <span><sup>3</sup> UC San Diego</span>
        </p>
      </div>
      <div class="hf-hero-actions">
        <a class="hf-button is-disabled" aria-disabled="true">Paper 路 Coming soon</a>
        <a class="hf-button is-disabled" aria-disabled="true">Code 路 Check back later</a>
      </div>
    </div>
  </section>

  <section class="hf-abstract">
    <div class="container">
      <h2 class="hf-section-title">Abstract</h2>
  <p class="hf-abstract-text">Controllable driving scene generation is critical for realistic and scalable autonomous driving simulation, yet existing approaches struggle to jointly achieve photorealism and precise control. We introduce <b>HorizonForge</b>, a unified framework that reconstructs scenes as editable Gaussian Splats and Meshes, enabling fine-grained 3D manipulation and language-driven vehicle insertion. Edits are rendered through a noise-aware video diffusion process that enforces spatial and temporal consistency, producing diverse scene variations in a single feed-forward pass without per-trajectory optimization. To standardize evaluation, we further propose <b>HorizonSuite</b>, a comprehensive benchmark spanning ego- and agent-level editing tasks such as trajectory modifications and object manipulation. Extensive experiments show that Gaussian-Mesh representation delivers substantially higher fidelity than alternative 3D representations, and that temporal priors from video diffusion are essential for coherent synthesis. Combining these finding, <b>HorizonForge</b> establishes a simple yet powerful paradigm for photorealistic, controllable driving simulation. achieving an 83.4% user-preference gain and a 25.19% FID improvement over the second best state-of-the-art method.</p>
    </div>
  </section>

  <section class="hf-method">
    <div class="container">
      <h2 class="hf-section-title">Method Overview</h2>
      <p class="hf-method-text">Overview of the <b>HorizonForge</b> framework. With original video and trajectory, we will firstly extract corresponding 3D assets according to the manipulated novel trajectories, then feed them into our rendering model for final generation results.</p>
      <img src="static/images/fig-trimmed.png" alt="Method Overview Figure" style="width: 80%; display: block; margin: 1rem auto 0; border-radius: 8px; border: 1px solid var(--hf-border);" />
    </div>
  </section>

  <section class="hf-demos">
    <div class="container">
      <div class="hf-demos-header">
        <h2 class="hf-section-title">Gallery</h2>
        <!-- <p class="hf-section-copy">Representative HorizonForge results organized by ego-control and other-agent edits.</p> -->
      </div>
      <div id="hf-demo-root" class="hf-demo-root"></div>
    </div>
  </section>
</div>

<footer class="hf-footer">
  <div class="container">
    <p>HorizonForge 路 Project page</p>
  </div>
</footer>

<script defer src="static/js/index.js"></script>
</body>
</html>
